{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "This jupyter notebook can be used to Query the Perf data with large result sets from Azure Log Analytics Workspace. \n",
    "\n",
    "It also has the KQL queries to collect Perf data from a Azure Log Analytics Workspace. The KQL queries relevant for CPU, Memory Usage data of pods, and their scaling parameters is included here. This notebook can be used to overcome the result export limitation from the Azure Log Analytics Workspace console.\n",
    "\n",
    "It has been modified from the original sample notebook provided by azure. https://github.com/Azure/azure-sdk-for-python/blob/azure-monitor-query_1.2.0/sdk/monitor/azure-monitor-query/samples/notebooks/sample_large_query.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install --upgrade azure-monitor-query azure-identity pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current date\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "date_time = now.strftime(\"%d-%m-%Y\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "An authenticated client is required to query data from Log Analytics. The following code shows how to create a `LogsQueryClient` using the `DefaultAzureCredential`. Note that an async credential and client are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity.aio import DefaultAzureCredential\n",
    "from azure.monitor.query.aio import LogsQueryClient\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "client = LogsQueryClient(credential)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Log Analytics workspace ID\n",
    "\n",
    "Set the `LOGS_WORKSPACE_ID` variable below to the ID of your Log Analytics workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGS_WORKSPACE_ID = \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper functions\n",
    "\n",
    "In order to overcome the service limits, the strategy is to query data in smaller chunks based on some time column (i.e. `TimeGenerated`). The following helper functions are useful for this by querying your data in order to find suitable start and end times for the batch queries.\n",
    "\n",
    "- The `get_batch_endpoints_by_row_count` function will return a list of times that can be used in the query time spans while ensuring that the number of rows returned will be less than the specified row limit. \n",
    "- The `get_batch_endpoints_by_size` function will return a list of times that can be used in the query time spans while ensuring that the size of the data returned is less than the specified byte size limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "from azure.monitor.query import LogsQueryStatus\n",
    "\n",
    "\n",
    "async def get_batch_endpoints_by_row_count(\n",
    "    query: str,\n",
    "    end_time: datetime, \n",
    "    days_back: int, \n",
    "    max_rows_per_query: int = int(1e5),\n",
    "    time_col: str = \"TimeGenerated\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Determine the timestamp endpoints for each chunked query\n",
    "    such that number of rows returned by each query is (approximately) `max_rows_per_query`\n",
    "    \"\"\"\n",
    "\n",
    "    # This query will assign a batch number to each row depending on the maximum number of rows per query.\n",
    "    # Then the earliest timestamp for each batch number is used for each query endpoint.\n",
    "    find_batch_endpoints_query = f\"\"\"\n",
    "        {query}\n",
    "        | sort by {time_col} desc\n",
    "        | extend batch_num = row_cumsum(1) / {max_rows_per_query}\n",
    "        | summarize endpoint=min({time_col}) by batch_num\n",
    "        | sort by batch_num asc\n",
    "        | project endpoint\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = end_time - timedelta(days=days_back)\n",
    "    try:\n",
    "        response = await client.query_workspace(\n",
    "            workspace_id=LOGS_WORKSPACE_ID,\n",
    "            query=find_batch_endpoints_query,\n",
    "            timespan=(start_time, end_time),\n",
    "        )\n",
    "    except HttpResponseError as e:\n",
    "        print(\"Error batching endpoints by row count\")\n",
    "        raise e\n",
    "\n",
    "    batch_endpoints = [end_time]\n",
    "    batch_endpoints += [row[0] for row in response.tables[0].rows]\n",
    "    return batch_endpoints\n",
    "\n",
    "\n",
    "async def get_batch_endpoints_by_byte_size(\n",
    "    query: str,\n",
    "    end_time: datetime, \n",
    "    days_back: int,\n",
    "    max_bytes_per_query: int = 100 * 1024 * 1024, # 100 MiB\n",
    "    time_col: str = \"TimeGenerated\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Determine the timestamp endpoints for each chunked query such that\n",
    "    the size of the data returned is less than `max_bytes_per_query`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # This query will assign a batch number to each row depending on the estimated data size.\n",
    "    # Then the earliest timestamp for each batch number is used for each query endpoint.\n",
    "    find_batch_endpoints_query = f\"\"\"\n",
    "        {query}\n",
    "        | sort by {time_col} desc\n",
    "        | extend batch_num = row_cumsum(estimate_data_size(*)) / {max_bytes_per_query}\n",
    "        | summarize endpoint=min({time_col}) by batch_num\n",
    "        | sort by batch_num asc\n",
    "        | project endpoint\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = end_time - timedelta(days=days_back)\n",
    "    try:\n",
    "        response = await client.query_workspace(\n",
    "            workspace_id=LOGS_WORKSPACE_ID,\n",
    "            query=find_batch_endpoints_query,\n",
    "            timespan=(start_time, end_time)\n",
    "        )\n",
    "    except HttpResponseError as e:\n",
    "        print(\"Error batching endpoints by byte size\")\n",
    "        raise e\n",
    "\n",
    "    batch_endpoints = [end_time]\n",
    "    batch_endpoints += [row[0] for row in response.tables[0].rows]\n",
    "    return batch_endpoints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a function that will asynchronously execute a given query over a time range specified by a given start time and end time. This function will call the `query_workspace` method of the `LogsQueryClient`. The Azure Monitor Query library will automatically handle retries in case of connection-related errors or server errors (i.e. 500, 503, and 504 status codes). Check [here](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/core/azure-core#configurations) for more information on configuring retries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_query(\n",
    "    query: str, \n",
    "    start_time: datetime, \n",
    "    end_time: datetime, \n",
    "    *, \n",
    "    query_id: str = \"\",\n",
    "    correlation_request_id: str = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Asynchronously execute the given query, restricted to the given time range, and parse the API response.\n",
    "\n",
    "    :param str query: The query to execute.\n",
    "    :param datetime start_time: The start of the time range to query.\n",
    "    :param datetime end_time: The end of the time range to query.\n",
    "    :keyword str query_id: Optional identifier for the query, used for printing.\n",
    "    :keyword str correlation_request_id, Optional correlation ID to use in the query headers for tracing.\n",
    "    \"\"\"\n",
    "    headers = {}\n",
    "    if correlation_request_id:\n",
    "        headers[\"x-ms-correlation-request-id\"] = correlation_request_id\n",
    "\n",
    "    try:\n",
    "        response = await client.query_workspace(\n",
    "            workspace_id=LOGS_WORKSPACE_ID,\n",
    "            query=query,\n",
    "            timespan=(start_time, end_time),\n",
    "            server_timeout=360,                 \n",
    "            include_statistics=False, # Can be used for debugging.\n",
    "            headers=headers,\n",
    "            retry_on_methods=[\"POST\"]\n",
    "        )\n",
    "    except HttpResponseError as e:\n",
    "        print(f\"Error when attempting query {query_id} (query time span: {start_time} to {end_time}):\\n\\t\", e)\n",
    "        return []\n",
    "\n",
    "    if response.status == LogsQueryStatus.SUCCESS:\n",
    "        print(f\"Query {query_id} successful (query time span: {start_time} to {end_time}). Row count: {len(response.tables[0].rows)}\")\n",
    "        return response.tables[0]\n",
    "    else:\n",
    "        # This will be a LogsQueryPartialResult.\n",
    "        error = response.partial_error\n",
    "        print(f\"Partial results returned for query {query_id} (query time span: {start_time} to {end_time}):\\n\\t\", error)\n",
    "        return response.partial_data[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query data\n",
    "\n",
    "With the helper functions defined, you can now query the data in chunks that won't hit the row count and data size service limits.\n",
    "\n",
    "### Set variables\n",
    "\n",
    "Before running the queries, some variables will need to be configured.\n",
    "\n",
    "- `QUERY` - KQL query to run. Change the table name and specify any required columns and filters as needed. When constructing this query, the recommendation is to use [reduced KQL](https://learn.microsoft.com/azure/azure-monitor/logs/basic-logs-query?tabs=portal-1#kql-language-limits) which are optimized for data retrieval. To get all rows/columns, just set `QUERY = <name-of-table>`. \n",
    "- `END_TIME` - End of the time range to query over.\n",
    "- `DAYS_BACK` - The number of days to go back from the end time. For example, if `END_TIME = datetime.now()` and `DAYS_BACK = 7`, the query will return data from the last 7 days. Note that fetched data will (initially) be stored in memory on your system, so it is possible to run into memory limitations if the query returns a large amount of data. If this issue is encountered, consider querying the data in time segments. For example, instead of querying 365 days of data at once, query 100 days of data at a time by setting the values of `END_TIME` and `DAYS_BACK` appropriately and re-running the notebook from this cell onwards for each separate segment.\n",
    "- `MAX_ROWS_PER_QUERY` - The max number of rows that is returned from a single query. This is defaulted to the service limit of 500,000 rows multiplied by some factor to allow for some wiggle room. This limit may sometimes be exceeded if many entries share the same timestamp.\n",
    "- `MAX_BYTES_PER_QUERY` - The max size in bytes of data returned from a single query. This is defaulted to the service limit of 100 MiB multiplied by some factor to allow for some wiggle room.\n",
    "- `MAX_CONCURRENT_QUERIES` - The max number of concurrent queries to run at once. This is defaulted to 5. Reducing this can help avoid errors due to rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS VALUE WITH YOUR QUERY.\n",
    "# If necessary, add a KQL `project` operator or any filtering operators to limit the number of rows returned.\n",
    "QUERY_CPU = \"\"\"\n",
    "let k8sNamespace = \"\";\n",
    "let subscriptionId = \"\";\n",
    "let resourceGroup = \"\";\n",
    "let clusterName = \"\";\n",
    "let metricName = \"cpuUsageNanoCores\";\n",
    "// let metricName = \"memoryWorkingSetBytes\";\n",
    "let clusterIdPrefix = strcat(\"/subscriptions/\", subscriptionId, \"/resourceGroups/\", resourceGroup, \"/providers/Microsoft.ContainerService/managedClusters/\", clusterName, \"/\");\n",
    "let ContainerNames = KubePodInventory\n",
    "// | where TimeGenerated >= startDateTime and TimeGenerated < endDateTime\n",
    "    | where ClusterName == clusterName\n",
    "    | where ControllerKind == \"ReplicaSet\"\n",
    "    | where PodStatus == \"Running\"\n",
    "    | where Namespace startswith_cs k8sNamespace\n",
    "    | distinct InstanceName=strcat(clusterIdPrefix, ContainerName), Name, PodCreationTimeStamp, Namespace, PodStartTime;\n",
    "let InstanceNames = ContainerNames\n",
    "| project InstanceName;\n",
    "Perf\n",
    "| where ObjectName == 'K8SContainer'\n",
    "| where InstanceName in (InstanceNames)\n",
    "| where CounterName == metricName\n",
    "| join kind=inner (ContainerNames) on InstanceName\n",
    "| project TimeGenerated, CpuCounter=CounterValue, InstanceName, Name, PodCreationTimeStamp, Namespace, PodStartTime\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_MEMORY = \"\"\"\n",
    "let k8sNamespace = \"\";\n",
    "let subscriptionId = \"\";\n",
    "let resourceGroup = \"\";\n",
    "let clusterName = \"\";\n",
    "// let metricName = \"cpuUsageNanoCores\";\n",
    "let metricName = \"memoryWorkingSetBytes\";\n",
    "let clusterIdPrefix = strcat(\"/subscriptions/\", subscriptionId, \"/resourceGroups/\", resourceGroup, \"/providers/Microsoft.ContainerService/managedClusters/\", clusterName, \"/\");\n",
    "let ContainerNames = KubePodInventory\n",
    "// | where TimeGenerated >= startDateTime and TimeGenerated < endDateTime\n",
    "    | where ClusterName == clusterName\n",
    "    | where ControllerKind == \"ReplicaSet\"\n",
    "    | where PodStatus == \"Running\"\n",
    "    | where Namespace startswith_cs k8sNamespace\n",
    "    | distinct InstanceName=strcat(clusterIdPrefix, ContainerName), Name, PodCreationTimeStamp, Namespace, PodStartTime;\n",
    "let InstanceNames = ContainerNames\n",
    "| project InstanceName;\n",
    "Perf\n",
    "// | where TimeGenerated >= startDateTime and TimeGenerated < endDateTime\n",
    "| where ObjectName == 'K8SContainer'\n",
    "| where InstanceName in (InstanceNames)\n",
    "| where CounterName == metricName\n",
    "| join kind=inner (\n",
    "ContainerNames\n",
    ") on InstanceName\n",
    "| project TimeGenerated, MemoryCounter=CounterValue, InstanceName, Name, PodCreationTimeStamp, Namespace, PodStartTime\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below queries needed to get the instance and scaling information can be run directly on the azure log analytics workspace and be exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on azure console\n",
    "QUERY_INSTANCES = \"\"\"\n",
    "let endDateTime = \"\"\n",
    "let startDateTime = \"\"\n",
    "let k8sNamespace = \"\";\n",
    "let subscriptionId = \"\";\n",
    "let resourceGroup = \"\";\n",
    "let clusterName = \"\";\n",
    "let clusterIdPrefix = strcat(\"/subscriptions/\", subscriptionId, \"/resourceGroups/\", resourceGroup, \"/providers/Microsoft.ContainerService/managedClusters/\", clusterName, \"/\");\n",
    "KubePodInventory\n",
    " | where TimeGenerated >= startDateTime and TimeGenerated < endDateTime\n",
    "    | where Namespace startswith_cs k8sNamespace\n",
    "    | where ControllerKind == \"ReplicaSet\"\n",
    "    | where PodStatus == \"Running\"\n",
    "    | distinct InstanceName=strcat(clusterIdPrefix, ContainerName), Name\n",
    " | join kind=inner hint.strategy=shuffle (\n",
    "    Perf\n",
    "    | where CounterName in (\"cpuLimitNanoCores\", \"cpuRequestNanoCores\", \"memoryRequestBytes\", \"memoryLimitBytes\")\n",
    "    | summarize Value = max(CounterValue) by InstanceName, CounterName\n",
    "    | project\n",
    "        InstanceName,\n",
    "        Value,\n",
    "        CounterName\n",
    "    )\n",
    "    on InstanceName\n",
    "| project InstanceName, Name, CounterName, Value \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_HPA = \"\"\"\n",
    "let endDateTime = \"\"\n",
    "let startDateTime = \"\"\n",
    "KubePodInventory\n",
    "| where TimeGenerated >= startDateTime \n",
    "| where ClusterName == \"\"\n",
    "| where Namespace startswith \"\"\n",
    "| extend labels = todynamic(PodLabel)\n",
    "| extend deployment_hpa = reverse(substring(reverse(ControllerName), indexof(reverse(ControllerName), \"-\") + 1))\n",
    "| distinct tostring(deployment_hpa)\n",
    "| join kind=inner (InsightsMetrics \n",
    "    | where TimeGenerated >= startDateTime and TimeGenerated < endDateTime\n",
    "    | where Name == 'kube_hpa_status_current_replicas'\n",
    "    | extend pTags = todynamic(Tags) //parse the tags for values\n",
    "    | extend ns = todynamic(pTags.k8sNamespace) //parse namespace value from tags\n",
    "    | extend deployment_hpa = todynamic(pTags.targetName) //parse HPA target name from tags\n",
    "    | extend last_scale_time = todynamic(pTags.lastScaleTime)\n",
    "    | extend min_reps = todynamic(pTags.spec_min_replicas)\n",
    "    | extend max_reps = todynamic(pTags.spec_max_replicas) // Parse maximum replica settings from HPA deployment\n",
    "    | extend desired_reps = todynamic(pTags.status_desired_replicas) // Parse desired replica settings from HPA deployment\n",
    "    | summarize arg_max(TimeGenerated, *) by tostring(ns), tostring(deployment_hpa), Cluster=toupper(tostring(split(_ResourceId, '/')[8])), toint(desired_reps), toint(max_reps), scale_out_percentage=(desired_reps * 100 / max_reps)\n",
    "    //| where scale_out_percentage > _minthreshold and scale_out_percentage <= _maxthreshold\n",
    "    )\n",
    "    on deployment_hpa\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you want to use a different end time, uncomment the following line and adjust as needed.\n",
    "END_TIME = datetime.strptime(\"2024-04-25 03:30:00 +0530\", \"%Y-%m-%d %H:%M:%S %z\")\n",
    "DAYS_BACK = 28\n",
    "\n",
    "MAX_ROWS_PER_QUERY_SERVICE_LIMIT = int(5e5)  # 500K\n",
    "MAX_ROWS_PER_QUERY = int(MAX_ROWS_PER_QUERY_SERVICE_LIMIT * 0.9)\n",
    "\n",
    "MAX_BYTES_PER_QUERY_SERVICE_LIMIT = 100 * 1024 * 1024 # 100 MiB of compressed data\n",
    "MAX_BYTES_PER_QUERY = int(MAX_BYTES_PER_QUERY_SERVICE_LIMIT * 0.6)\n",
    "\n",
    "MAX_CONCURRENT_QUERIES = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate data and costs (optional)\n",
    "\n",
    "Before running the chunked queries, it might first be prudent to estimate the size of the data if planning on exporting the query results to another service. The below cell defines another helper function that can be used to estimate the size of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def estimate_data_size(query: str, end_time: datetime, days_back: int):\n",
    "    query = f\"{query} | summarize n_rows = count(), estimate_data_size = sum(estimate_data_size(*))\"\n",
    "    start_time = end_time - timedelta(days=days_back)\n",
    "    response = await client.query_workspace(\n",
    "        workspace_id=LOGS_WORKSPACE_ID,\n",
    "        query=query,\n",
    "        timespan=(start_time, end_time),\n",
    "    )\n",
    "\n",
    "    columns = response.tables[0].columns\n",
    "    rows = response.tables[0].rows\n",
    "    df = pd.DataFrame(data=rows, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the following cell to estimate the size of the data that will be returned by the queries. Note that this is just an estimate and the actual size may vary slightly. This information can be used in conjunction with the Azure storage [pricing calculator](https://azure.microsoft.com/pricing/calculator/?service=storage) to determine costs that will be incurred for your storage setup. If using Azure Data Lake Storage Gen2, full billing details can be found [here](https://azure.microsoft.com/pricing/details/storage/data-lake/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size_df_cpu = await estimate_data_size(QUERY_CPU, END_TIME, DAYS_BACK)\n",
    "data_size_df_cpu[\"estimate_data_size_MB\"] = data_size_df_cpu[\"estimate_data_size\"] / (1000 **2)\n",
    "data_size_df_cpu[\"estimate_data_size_GB\"] = data_size_df_cpu[\"estimate_data_size_MB\"] / 1000\n",
    "data_size_df_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size_df_memory = await estimate_data_size(QUERY_MEMORY, END_TIME, DAYS_BACK)\n",
    "data_size_df_memory[\"estimate_data_size_MB\"] = data_size_df_memory[\"estimate_data_size\"] / (1000 **2)\n",
    "data_size_df_memory[\"estimate_data_size_GB\"] = data_size_df_memory[\"estimate_data_size_MB\"] / 1000\n",
    "data_size_df_memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch log data\n",
    "\n",
    "Use the helper functions to create an async wrapper function that will query the data in chunks using the variables defined above. This function will return the results as a single pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import itertools\n",
    "import uuid\n",
    "\n",
    "# Limit the number of concurrent queries.\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_QUERIES)\n",
    "\n",
    "async def fetch_logs(query: str, start_time: datetime, end_time: datetime, query_id: str, correlation_request_id: str):\n",
    "    async with semaphore:\n",
    "        return await execute_query(query, start_time, end_time, query_id=query_id, correlation_request_id=correlation_request_id)\n",
    "\n",
    "\n",
    "async def run(QUERY):\n",
    "    # Below, we combine the endpoints retrieved from both endpoint methods to ensure that the number of rows\n",
    "    # and the size of the data returned are both within the limits.\n",
    "    # Worst case performance is double the theoretical minimum number of queries necessary.\n",
    "    row_batch_endpoints = await get_batch_endpoints_by_row_count(QUERY, END_TIME, days_back=DAYS_BACK, max_rows_per_query=MAX_ROWS_PER_QUERY)\n",
    "    byte_batch_endpoints = await get_batch_endpoints_by_byte_size(QUERY, END_TIME, days_back=DAYS_BACK, max_bytes_per_query=MAX_BYTES_PER_QUERY)\n",
    "    batch_endpoints = sorted(set(byte_batch_endpoints + row_batch_endpoints), reverse=True)\n",
    "\n",
    "    queries = []\n",
    "    end_time = batch_endpoints[0]\n",
    "    correlation_request_id = str(uuid.uuid4())\n",
    "\n",
    "    print(f\"Querying {len(batch_endpoints) - 1} time ranges, from {batch_endpoints[-1]} to {end_time}\")\n",
    "    print(f\"Correlation request ID: {correlation_request_id}\")\n",
    "    \n",
    "    for i in range(1, len(batch_endpoints)):\n",
    "        start_time = batch_endpoints[i]\n",
    "        queries.append(fetch_logs(QUERY, start_time, end_time, query_id=str(i), correlation_request_id=correlation_request_id))\n",
    "        end_time = start_time - timedelta(microseconds=1) # Subtract 1 microsecond to avoid overlap between queries.\n",
    "\n",
    "    responses = await asyncio.gather(*queries)\n",
    "\n",
    "    rows = itertools.chain.from_iterable([table.rows for table in responses if table])\n",
    "    columns = responses[0].columns\n",
    "    return pd.DataFrame(data=rows, columns=columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, go ahead and run the following cell to fetch the data. Note that this may take some time depending on the size of the data and the number of queries that need to be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpu = await run(QUERY_CPU)\n",
    "print(f\"Retrieved {len(df_cpu)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "df_cpu.to_csv('./data/raw/cpu/cpu-{}.csv'.format(date_time), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_memory = await run(QUERY_MEMORY)\n",
    "print(f\"Retrieved {len(df_memory)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "df_memory.to_csv('./data/raw/memory/memory-{}.csv'.format(date_time), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data has been fetched, you can now use the `df` DataFrame for further analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "665f5865bb085838e35a9597206be80722fad7fd0d11e0dfbe620869aad35c71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
