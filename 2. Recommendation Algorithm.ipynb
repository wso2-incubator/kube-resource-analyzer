{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPU_CONSTANT = 1e9\n",
    "MEM_CONSTANT = 1.048576e+6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data_date = \"25-04-2024\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU & Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_cpu = pd.read_csv(f'./data/raw/cpu/cpu-{source_data_date}.csv')\n",
    "df_main_memory = pd.read_csv(f'./data/raw/memory/memory-{source_data_date}.csv')\n",
    "\n",
    "#for both df_main_cpu and df_main_memory make the timeGenerated to pandas and component to the component name\n",
    "df_main_cpu['TimeGenerated'] = pd.to_datetime(df_main_cpu['TimeGenerated'])\n",
    "df_main_cpu['Component'] = df_main_cpu['Name'].str.split('-').str[:-3].str.join('-')\n",
    "df_main_cpu['CpuCounter'] /= CPU_CONSTANT\n",
    "\n",
    "df_main_memory['TimeGenerated'] = pd.to_datetime(df_main_memory['TimeGenerated'])\n",
    "df_main_memory['Component'] = df_main_memory['Name'].str.split('-').str[:-3].str.join('-')\n",
    "df_main_memory['MemoryCounter'] /= MEM_CONSTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instances = pd.read_csv(f'./data/raw/instances/instances-{source_data_date}.csv')\n",
    "\n",
    "def clean_data(df_instances):\n",
    "    df_instances = df_instances.pivot_table(index=['Name'], columns='CounterName', values='Value', fill_value=0) #fills missing values with 0\n",
    "    df_instances = df_instances.reset_index()\n",
    "    df_instances.drop_duplicates()\n",
    "    df_instances[\"cpuLimitNanoCores\"] /= CPU_CONSTANT\n",
    "    df_instances[\"cpuRequestNanoCores\"] /=CPU_CONSTANT\n",
    "    df_instances[\"memoryLimitBytes\"] /=MEM_CONSTANT\n",
    "    df_instances[\"memoryRequestBytes\"] /=MEM_CONSTANT\n",
    "\n",
    "    df_instances = df_instances.rename(columns={'cpuLimitNanoCores': 'cpuLimitCores','cpuRequestNanoCores': 'cpuRequestCores','memoryLimitBytes': 'memoryLimitMiB','memoryRequestBytes': 'memoryRequestMiB'})\n",
    "    return df_instances\n",
    "\n",
    "df_instances = clean_data(df_instances.copy())\n",
    "df_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hpa = pd.read_csv(f'./data/raw/instances/hpa-{source_data_date}.csv')\n",
    "\n",
    "def clean_data(df_hpa):\n",
    "    # Filter rows based on column: 'ns'\n",
    "    df_hpa = df_hpa[df_hpa['ns'].str.startswith(\"dp-staging\", na=False)]\n",
    "    # Drop column: 'Cluster'\n",
    "    df_hpa = df_hpa.drop(columns=['Cluster', 'deployment_hpa2','scale_out_percentage','Computer', 'Origin', 'Namespace', 'Name',\n",
    "    'AgentId', 'Tags', 'TimeGenerated [Sri Jayawardenepura]', 'Type', '_ResourceId', 'pTags', 'Val'])\n",
    "    # Drop duplicate rows across all columns\n",
    "    df_hpa = df_hpa.drop_duplicates()\n",
    "    # if there is a missing value for last_scale_time fill it with zero, make the others 1\n",
    "    df_hpa['last_scale_time'] = df_hpa['last_scale_time'].fillna(0)\n",
    "    #values other than zero are converted to 1\n",
    "    df_hpa['last_scale_time'] = df_hpa['last_scale_time'].apply(lambda x: 1 if x != 0 else 0)\n",
    "    df_hpa['deployment_hpa'] = df_hpa['deployment_hpa'].str.split('-').str[:-1].str.join('-')\n",
    "    df_hpa['ns'] = df_hpa['ns'].str.split('-').str[2:-2].str.join('-')\n",
    "    # Change the column name to last_scaled\n",
    "    df_hpa = df_hpa.rename(columns={'last_scale_time': 'LastScaled', 'ns': 'Namespace', 'deployment_hpa': 'Component', 'desired_reps': 'DesiredReplicas', 'min_reps': 'MinReplicas', 'max_reps': 'MaxReplicas'})\n",
    "    return df_hpa\n",
    "\n",
    "df_hpa = clean_data(df_hpa.copy())\n",
    "df_hpa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Pod Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pod_duration(df_main_cpu):\n",
    "    df_pod_duration = df_main_cpu.groupby('Name').agg({'TimeGenerated': ['min', 'max'], 'Component': 'first'})\n",
    "    df_pod_duration['TimeDifference'] = df_pod_duration['TimeGenerated']['max'] - df_pod_duration['TimeGenerated']['min']\n",
    "    df_pod_duration.reset_index(inplace=True)\n",
    "    df_pod_duration.columns = ['Name', 'TimeStamp_min', 'TimeStamp_max', 'Component','TimeDifference']\n",
    "    return df_pod_duration\n",
    "\n",
    "df_pod_duration = pod_duration(df_main_cpu.copy())\n",
    "df_pod_duration.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Unique Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make it into a dataframe\n",
    "# unique_components = pd.DataFrame(df_pod_duration['Component'].unique(), columns=['Component'])\n",
    "# #make this single column into a single row\n",
    "# unique_components = unique_components.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_instances.merge(df_pod_duration, on='Name', how='left')\n",
    "df_merged = df_merged.merge(df_hpa, on='Component', how='left')\n",
    "df_merged = df_merged.dropna(subset=['TimeDifference', 'Component'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inner Join CPU and Memory Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inner Join (Note: Inner and Outerjoin gave same results)\n",
    "df_main_inner = df_main_cpu.merge(df_main_memory, on=['TimeGenerated', 'Name'], how='inner', suffixes=('', '_memory'))\n",
    "df_main_inner = df_main_inner.merge(df_merged, on='Name', how='inner', suffixes=('', '_instances'))\n",
    "\n",
    "def clean_data(df_main_inner):\n",
    "    # Drop columns: 'Component_memory', 'PodStartTime_memory' and 3 other columns\n",
    "    df_main_inner = df_main_inner.drop(columns=['Component_memory', 'PodStartTime_memory', 'Namespace_memory', 'PodCreationTimeStamp_memory', 'InstanceName_memory', 'Namespace_instances', 'Component_instances'])\n",
    "    return df_main_inner\n",
    "\n",
    "df_main_inner = clean_data(df_main_inner.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_recommender(data, instances,  window='3d', offset='0d', algorithmCPU='Max', algorithmRAM='Quantile', qCPU=0.95, qRAM=0.95,  targetCPUUtilization=0.7, targetRAMUtilization=0.8, minRecCpuCores=0.01, minRecRamMiB=20, cpu_limit_factor=1.5, mem_limit_factor=1.5):\n",
    "    # Ensure 'TimeGenerated' is in datetime format\n",
    "    data['TimeGenerated'] = pd.to_datetime(data['TimeGenerated'])\n",
    "    instances['TimeDifference'] = pd.to_timedelta(instances['TimeDifference'])   \n",
    "    \n",
    "    instances_data = instances.copy()\n",
    "    instances_data = instances_data.dropna(subset=['TimeDifference', 'Component', 'MinReplicas'])\n",
    "    \n",
    "        # Define the lambda functions for CPU and RAM calculations\n",
    "    cpu_func = lambda x: x.max() if algorithmCPU == 'Max' else x.quantile(qCPU)\n",
    "    ram_func = lambda x: x.max() if algorithmRAM == 'Max' else x.quantile(qRAM)\n",
    "    \n",
    "    components = instances['Component'].unique()\n",
    "    updates = []\n",
    "    for component in components:\n",
    "        # Filter data based on component\n",
    "        component_data = instances_data[instances_data['Component'] == component]\n",
    "        # check which have a TimeDifference greater than the window\n",
    "        component_data = component_data[component_data['TimeDifference'] >= pd.Timedelta(window)]\n",
    "        if component_data.empty:\n",
    "            continue\n",
    "        main_data = data[data['Component'] == component]\n",
    "        #get the most recent time in the data - offset\n",
    "        end_time = main_data['TimeGenerated'].max()- pd.Timedelta(offset)\n",
    "        #limit the data to the window\n",
    "        main_data = main_data[(main_data['TimeGenerated'] >= end_time - pd.Timedelta(window)) & (main_data['TimeGenerated'] <= end_time)]\n",
    "\n",
    "        # Group by 'Name' for each component to reduce computations\n",
    "        grouped_data = main_data.groupby('Name').agg(\n",
    "            base_cpu_recommendation=('CpuCounter', cpu_func),\n",
    "            base_mem_recommendation=('MemoryCounter', ram_func)\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Apply the calculations\n",
    "        grouped_data['RecommendedCpuRequestCores'] = grouped_data['base_cpu_recommendation'].apply(lambda x: max(round(x / targetCPUUtilization, 3), minRecCpuCores))\n",
    "        grouped_data['RecommendedMemoryRequestMiB'] = grouped_data['base_mem_recommendation'].apply(lambda x: max(round(x / targetRAMUtilization, 0), minRecRamMiB))\n",
    "        grouped_data['RecommendedCpuLimitCores'] = grouped_data['RecommendedCpuRequestCores'].apply(lambda x: round(x * cpu_limit_factor, 3))\n",
    "        grouped_data['RecommendedMemoryLimitMiB'] = grouped_data['RecommendedMemoryRequestMiB'].apply(lambda x: round(x * mem_limit_factor, 0))\n",
    "        \n",
    "        #concatenate the grouped data\n",
    "        updates.append(grouped_data[['Name', 'RecommendedCpuRequestCores', 'RecommendedMemoryRequestMiB', 'RecommendedCpuLimitCores', 'RecommendedMemoryLimitMiB']])\n",
    "        \n",
    "    updates_df = pd.concat(updates, ignore_index=True)\n",
    "    instances_data = pd.merge(instances, updates_df, on='Name', how='left')\n",
    "    \n",
    "    #group instances by 'Component' and aggregate by the max of 'cpuRequestCores', 'cpuLimitCores', 'memoryRequestMiB', 'memoryLimitMiB', 'MinReplicas', 'MaxReplicas', 'RecommendedCpuRequestCores', 'RecommendedCpuLimitCores', 'RecommendedMemoryRequestMiB', 'RecommendedMemoryLimitMiB'\n",
    "    instances_data = instances_data.groupby('Component').agg(\n",
    "        Namespace=('Namespace', 'first'),\n",
    "        cpuRequestCores=('cpuRequestCores', 'max'),\n",
    "        cpuLimitCores=('cpuLimitCores', 'max'),\n",
    "        memoryRequestMiB=('memoryRequestMiB', 'max'),\n",
    "        memoryLimitMiB=('memoryLimitMiB', 'max'),\n",
    "        MinReplicas=('MinReplicas', 'max'),\n",
    "        MaxReplicas=('MaxReplicas', 'max'),\n",
    "        RecommendedCpuRequestCores=('RecommendedCpuRequestCores', 'max'),\n",
    "        RecommendedCpuLimitCores=('RecommendedCpuLimitCores', 'max'),\n",
    "        RecommendedMemoryRequestMiB=('RecommendedMemoryRequestMiB', 'max'),\n",
    "        RecommendedMemoryLimitMiB=('RecommendedMemoryLimitMiB', 'max')\n",
    "    ).reset_index()\n",
    "    instances_data = instances_data.dropna(subset=['RecommendedCpuRequestCores', 'RecommendedCpuLimitCores', 'RecommendedMemoryRequestMiB', 'RecommendedMemoryLimitMiB'])\n",
    "    return instances_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = ['3d', '7d']\n",
    "offsets = ['0d', '1d', '2d']\n",
    "\n",
    "for window in windows:\n",
    "    for offset in offsets:\n",
    "        df_recommended = optimized_recommender(df_main_inner, df_merged, window=window, offset=offset, algorithmCPU='Max', algorithmRAM='Quantile', qCPU=0.9999, qRAM=0.95,  targetCPUUtilization=0.7, targetRAMUtilization=0.8, minRecCpuCores=0.1, minRecRamMiB=20, cpu_limit_factor=2, mem_limit_factor=2)\n",
    "        df_recommended.to_csv(f'./reports/shifting-time-period/Recommendations-{source_data_date}-{window}-{offset}Offset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
